{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "# from mlagents.envs import UnityEnvironment\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "environment_path = \"Environments/Banana.app\"\n",
    "env = UnityEnvironment(file_name=environment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [0.         1.         0.         0.         0.16895212 0.\n",
      " 1.         0.         0.         0.20073597 1.         0.\n",
      " 0.         0.         0.12865657 0.         1.         0.\n",
      " 0.         0.14938059 1.         0.         0.         0.\n",
      " 0.58185619 0.         1.         0.         0.         0.16089135\n",
      " 0.         1.         0.         0.         0.31775284 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_QNetwork(nn.Module):\n",
    "    def __init__(self,state_space,action_space,seed,hidden_dims=(32,32),activation_fc=F.relu):\n",
    "        super(Dueling_QNetwork,self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        print('hidden_dims',hidden_dims)\n",
    "        self.input_layer = nn.Linear(state_space,int(hidden_dims[0]))\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.value_output = nn.Linear(hidden_dims[-1],1)\n",
    "        self.advantage_output = nn.Linear(hidden_dims[-1],action_space)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        a = self.advantage_output(x)\n",
    "        v = self.value_output(x)\n",
    "        v = v.expand_as(a)\n",
    "        q = v + a - a.mean(1,keepdim=True).expand_as(a)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Tree.\n",
    "3 tiered tree structure containing\n",
    "Root node (Object. sum of all lower values)\n",
    "Intermediate Node (Object. Root as parent, sums a given slice of the priority array)\n",
    "Priority Array (Array of priorities, length buffer_size)\n",
    "\n",
    "The number of Intermediate nodes is calculated by the buffer_size / batch_size.\n",
    "\n",
    "I_episode: current episode of training\n",
    "\n",
    "Index: is calculated by i_episode % buffer_size. This loops the index after exceeding the buffer_size.\n",
    "\n",
    "Indices: (List) of memory/priority entries\n",
    "\n",
    "intermediate_dict: maps index to intermediate node. Since each Intermediate node is responsible \n",
    "for a given slice of the priority array, given a particular index, it will return the Intermediate node\n",
    "'responsible' for that index.\n",
    "\n",
    "## Functions:\n",
    "\n",
    "Add:\n",
    "Calculates the priority of each TD error -> (abs(TD_error)+epsilon)**alpha\n",
    "Stores the priority in the Priority_array.\n",
    "Updates the sum_tree with the new priority\n",
    "\n",
    "Update_Priorities:\n",
    "Updates the index with the latest priority of that sample. As priorities can change over training\n",
    "for a particular experience\n",
    "\n",
    "Sample:\n",
    "Splits the current priority_array based on the number of entries, by the batch_size.\n",
    "Returns the indicies of those samples and the priorities.\n",
    "\n",
    "Propogate:\n",
    "Propogates the new priority value up through the tree\n",
    "\"\"\"\n",
    "\n",
    "class PriorityTree(object):\n",
    "    def __init__(self,buffer_size,batch_size,alpha,epsilon):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_intermediate_nodes = math.ceil(buffer_size / batch_size)\n",
    "        self.current_intermediate_node = 0\n",
    "        self.root = Node(None)\n",
    "        self.intermediate_nodes = [Intermediate(self.root,batch_size*x,batch_size*(x+1)) for x in range(self.num_intermediate_nodes)]\n",
    "        self.priority_array = np.zeros(buffer_size)\n",
    "        self.intermediate_dict = {}\n",
    "        for index,node in enumerate(self.intermediate_nodes):\n",
    "            for key in range((batch_size*(index+1))-batch_size,batch_size*(index+1)):\n",
    "                self.intermediate_dict[key] = node\n",
    "        print('Priority Tree: Batch Size {} Buffer size {} Number of intermediate Nodes {}'.format(batch_size,buffer_size,self.num_intermediate_nodes))\n",
    "        \n",
    "    def add(self,TD_error,index):\n",
    "        priority = (abs(TD_error)+self.epsilon)**self.alpha\n",
    "        self.priority_array[index] = priority\n",
    "        # Update sum\n",
    "        propogate(self.intermediate_dict[index],self.priority_array)\n",
    "    \n",
    "    def sample(self,index):\n",
    "        # Sample one experience uniformly from each slice of the priorities\n",
    "        if index >= self.buffer_size:\n",
    "            indicies = [random.sample(list(range(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes)),1)[0] for sample in range(self.batch_size)]\n",
    "        else:\n",
    "            interval = int(index / self.batch_size)\n",
    "            indicies = [random.sample(list(range(sample*interval,(sample+1)*interval)),1)[0] for sample in range(self.batch_size)]\n",
    "#         print('indicies',indicies)\n",
    "        priorities = self.priority_array[indicies]\n",
    "        return priorities,indicies\n",
    "    \n",
    "    def update_priorities(self,TD_errors,indicies):\n",
    "#         print('TD_errors',TD_errors)\n",
    "#         print('TD_errors shape',TD_errors.shape)\n",
    "        priorities = (abs(TD_errors)+self.epsilon)**self.alpha\n",
    "#         print('priorities shape',priorities.shape)\n",
    "#         print('indicies shape',len(indicies))\n",
    "#         print('self.priority_array shape',self.priority_array.shape)\n",
    "        self.priority_array[indicies] = priorities\n",
    "        # Update sum\n",
    "        nodes = [self.intermediate_dict[index] for index in indicies] \n",
    "        intermediate_nodes = set(nodes)\n",
    "        [propogate(node,self.priority_array) for node in intermediate_nodes]\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self,parent):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "            \n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def sum_children(self):\n",
    "        return sum([child.value for child in self.children])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "class Intermediate(Node):\n",
    "    def __init__(self,parent,start,end):\n",
    "        self.parent = parent\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.value = 0\n",
    "        parent.add_child(self)\n",
    "    \n",
    "    def sum_leafs(self,arr):\n",
    "        return np.sum(arr[self.start:self.end])\n",
    "\n",
    "def propogate(node,arr):\n",
    "    if node.parent != None:\n",
    "        node.value = node.sum_leafs(arr)\n",
    "        propogate(node.parent,arr)\n",
    "    else:\n",
    "        node.value = node.sum_children()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Buffer HyperParameters\n",
    "alpha(priority or w) dictates how biased the sampling should be towards the TD error. 0 < a < 1\n",
    "beta(IS) informs the importance of the sample update\n",
    "\n",
    "The paper uses a sum tree to calculate the priority sum in O(log n) time. As such, i've implemented my own version\n",
    "of the sum_tree which i call priority tree.\n",
    "\n",
    "We're increasing beta(IS) from 0.5 to 1 over time\n",
    "alpha(priority) we're holding constant at 0.5\n",
    "\"\"\"\n",
    "\n",
    "class PriorityReplayBuffer(object):\n",
    "    def __init__(self,action_size,buffer_size,batch_size,seed,alpha=0.5,beta=0.5,beta_end=1,beta_duration=1e+5,epsilon=7e-5):\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_duration = beta_duration\n",
    "        self.beta_increment = (beta_end - beta) / beta_duration\n",
    "        self.max_w = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.TD_sum = 0\n",
    "\n",
    "        self.experience = namedtuple('experience',field_names=['state','action','reward','next_state','done','i_episode'])\n",
    "        self.sum_tree = PriorityTree(buffer_size,batch_size,alpha,epsilon)\n",
    "        self.memory = {}\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done,TD_error,i_episode):\n",
    "        e = self.experience(state,action,reward,next_state,done,i_episode)\n",
    "        index = i_episode % self.buffer_size\n",
    "        # add memory to memory and add corresponding priority to the priority tree\n",
    "        self.memory[index] = e\n",
    "        self.sum_tree.add(TD_error,index)\n",
    "\n",
    "    def sample(self,index):\n",
    "        # We times the error by these weights for the updates\n",
    "        # Super inefficient to sum everytime. We could implement the tree sum structure. \n",
    "        # Or we could sum once on the first sample and then keep track of what we add and lose from the buffer.\n",
    "        # priority^a over the sum of the priorities^a = likelyhood of the given choice\n",
    "        # Anneal beta\n",
    "        self.update_beta()\n",
    "        # Get the samples and indicies\n",
    "        priorities,indicies = self.sum_tree.sample(index)\n",
    "        # Normalize with the sum\n",
    "        norm_priorities = priorities / self.sum_tree.root.value\n",
    "        samples = [self.memory[index] for index in indicies]\n",
    "#         samples = list(operator.itemgetter(*self.memory)(indicies))\n",
    "#         samples = self.memory[indicies]\n",
    "        # Importance weights\n",
    "#         print('self.beta',self.beta)\n",
    "#         print('self.beta',self.buffer_size)\n",
    "        importances = [(priority * self.buffer_size)**-self.beta for priority in norm_priorities]\n",
    "        self.max_w = max(self.max_w,max(importances))\n",
    "        # Normalize importance weights\n",
    "#         print('importances',importances)\n",
    "#         print('self.max_w',self.max_w)\n",
    "        norm_importances = [importance / self.max_w for importance in importances]\n",
    "#         print('norm_importances',norm_importances)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in samples if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in samples if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in samples if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in samples if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in samples if e is not None])).float().to(device)\n",
    "        \n",
    "        if index % 4900 == 0:\n",
    "            print('beta',self.beta)\n",
    "            print('self.max_w',self.max_w)\n",
    "            print('len mem',len(self.memory))\n",
    "            print('tree sum',self.sum_tree.root.value)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones),indicies,norm_importances\n",
    "\n",
    "    def update_beta(self):\n",
    "#         print('update_beta')\n",
    "#         print('self.beta_end',self.beta_end)\n",
    "#         print('self.beta_increment',self.beta_increment)\n",
    "        self.beta += self.beta_increment\n",
    "        self.beta = min(self.beta,self.beta_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DQN with Priority Replay, DDQN, and Dueling DQN.\n",
    "\"\"\"\n",
    "class Priority_DQN(object):\n",
    "    def __init__(self,state_space,action_space,seed,update_every,batch_size,buffer_size,min_buffer_size,learning_rate,GAMMA,tau,clip_norm,alpha):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        self.GAMMA = GAMMA\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.clip_norm = clip_norm\n",
    "        \n",
    "        self.qnetwork_local = Dueling_QNetwork(state_space,action_space,seed)\n",
    "        self.qnetwork_target = Dueling_QNetwork(state_space,action_space,seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=learning_rate)\n",
    "        # Initialize replaybuffer\n",
    "        self.memory = PriorityReplayBuffer(action_space,buffer_size,batch_size,seed,alpha)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self,state,action,reward,next_state,done,index):\n",
    "        # Calculate TD error\n",
    "        # Target\n",
    "        current_network_action = self.qnetwork_local(next_state).max(1)[1]\n",
    "        # initial state comes in as (1,4), squeeze to get (4)\n",
    "        target = reward + self.GAMMA*(self.qnetwork_target(next_state).squeeze(0)[current_network_action])\n",
    "        # Local. same rational for squeezing\n",
    "        local = self.qnetwork_local(state).squeeze(0)[action]\n",
    "        TD_error = reward + target - local\n",
    "        # Save the experience\n",
    "        self.memory.add(state,action,reward,next_state,done,TD_error,index)\n",
    "        \n",
    "        # learn from the experience\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.min_buffer_size:\n",
    "                experiences,indicies,weights = self.memory.sample(index)\n",
    "                self.learn(experiences,indicies,weights)\n",
    "        \n",
    "    def act(self,state,eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_space))\n",
    "        \n",
    "    def learn(self,experiences,indicies,weights):\n",
    "        \n",
    "        states,actions,rewards,next_states,dones = experiences\n",
    "        # Local max action\n",
    "        local_next_state_actions = self.qnetwork_local(next_states).max(1)[1].unsqueeze(1)\n",
    "        # Target\n",
    "        target_values = self.qnetwork_target(next_states).detach()\n",
    "        max_target = target_values.gather(1,local_next_state_actions)\n",
    "#         print('max_target size',max_target.size())\n",
    "        max_target *= (1-dones) \n",
    "        targets = rewards + (self.GAMMA*max_target)\n",
    "#         print('targets',targets.size())\n",
    "#         targets = rewards + self.GAMMA*(target_values.gather(1,local_next_state_actions))\n",
    "        # Local\n",
    "        local = self.qnetwork_local(states).gather(1,actions)\n",
    "        TD_error = local - targets\n",
    "        loss = ((torch.tensor(weights) * TD_error)**2*0.5).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(),self.clip_norm)\n",
    "        self.optimizer.step()\n",
    "        # Update the priorities\n",
    "        TD_errors = np.abs(TD_error.squeeze(1).detach().cpu().numpy())\n",
    "        self.memory.sum_tree.update_priorities(TD_errors,indicies)\n",
    "        self.update_target()\n",
    "\n",
    "    # def update_target(self,tau):\n",
    "    #     for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "    #         target_param.data.copy_(local_param.data)\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        self.memory = PriorityReplayBuffer(self.action_space,self.buffer_size,self.batch_size,self.seed,self.alpha)\n",
    "        \n",
    "    # Polyak averaging  \n",
    "    def update_target(self):\n",
    "        for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)\n",
    "#         self.qnetwork_local.parameters() = TAU*self.qnetwork_local.parameters() + (1-TAU)*self.qnetwork_target.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def train(agent,env,brain_name,checkpoint_path,n_episodes=1800, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        Instead of updating target every (int) steps, using Polyak updating of .1 to gradually merge the networks\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    index = 0\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state,action,reward,next_state,done,index)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            index += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps*eps_decay,eps_end)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), checkpoint_path)\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "MIN_BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 50\n",
    "ALPHA = 0.6 # 0.7 or 0.6\n",
    "START_BETA = 0.5 # from 0.5-1\n",
    "END_BETA = 1\n",
    "LR = 0.00025\n",
    "EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.01\n",
    "UPDATE_EVERY = 4\n",
    "CLIP_NORM = 10\n",
    "checkpoint_path = 'Vector_banana/vector_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space 37, Action Space 4\n",
      "hidden_dims (32, 32)\n",
      "hidden_dims (32, 32)\n",
      "Priority Tree: Batch Size 50 Buffer size 10000 Number of intermediate Nodes 200\n",
      "Episode 100\tAverage Score: 1.25\n",
      "Episode 200\tAverage Score: 4.55\n",
      "Episode 300\tAverage Score: 7.73\n",
      "Episode 400\tAverage Score: 9.42\n",
      "Episode 500\tAverage Score: 11.68\n",
      "Episode 600\tAverage Score: 12.11\n",
      "Episode 632\tAverage Score: 13.00\n",
      "Environment solved in 632 episodes!\tAverage Score: 13.00\n"
     ]
    }
   ],
   "source": [
    "nA = brain.vector_action_space_size\n",
    "nS = env_info.vector_observations.shape[1]\n",
    "print('Observation Space {}, Action Space {}'.format(nS,nA))\n",
    "seed = 11\n",
    "agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA)\n",
    "\n",
    "scores = train(agent,env,brain_name,checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "# Rolling mean plot\n",
    "interval = 25\n",
    "rolling_mean = [np.mean(scores[(slice_*interval):(slice_+1)*interval]) for slice_ in range(math.ceil(len(scores)/interval))]\n",
    "x_axis = np.arange(len(rolling_mean)) * interval\n",
    "plt.plot(x_axis, rolling_mean)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "# plt.show()\n",
    "plt.savefig('vector_banana_scores.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority Tree: Batch Size 50 Buffer size 10000 Number of intermediate Nodes 200\n"
     ]
    },
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-b5b0a4ed124f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvector_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAllBrainInfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "\n",
    "# try:\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load(checkpoint_path))\n",
    "#     agent.reset_memory()\n",
    "# except:  \n",
    "#     # Else reinstantiate the environment and other variablescheckpoint_path = 'vector_checkpoint.pth'\n",
    "#     nA = brain.vector_action_space_size\n",
    "#     nS = env_info.vector_observations.shape[1]\n",
    "#     print('Observation Space {}, Action Space {}'.format(nS,nA))\n",
    "#     seed = 11\n",
    "#     agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA)\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "# n_episodes = 1\n",
    "# max_t = 200\n",
    "# for i in range(1):\n",
    "#     index = 0\n",
    "#     for i_episode in range(1,n_episodes+1):\n",
    "#         env_info = env.reset(train_mode=True)[brain_name]\n",
    "#         state = env_info.vector_observations[0]\n",
    "#         score = 0\n",
    "# #         img = plt.imshow(np.squeeze(env_info.visual_observations[0]))\n",
    "#         for t in range(max_t):\n",
    "#             action = agent.act(state,0)\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             agent.step(state,action,reward,next_state,done,index)\n",
    "#             state = next_state\n",
    "#             index += 1\n",
    "#             score += reward\n",
    "#             # save the image\n",
    "            \n",
    "# #             img.set_data(np.squeeze(env_info.visual_observations[0])) \n",
    "# #             plt.savefig('banana_'+ str(index)+'.png',bbox_inches='tight')\n",
    "#             if done:\n",
    "#                 break\n",
    "#         print(score)\n",
    "#         if done:\n",
    "#             break \n",
    "            \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a GIF from stored sequence of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import imageio\n",
    "# # Make gif from images\n",
    "# path = '/Users/morgan/RL_gif_images'\n",
    "# images = []\n",
    "# filenames = ['test' + str(i) + '.png' for i in range(500)]\n",
    "# for filename in filenames:\n",
    "#     images.append(imageio.imread(path +'/'+ filename))\n",
    "# imageio.mimsave('Navigation.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
