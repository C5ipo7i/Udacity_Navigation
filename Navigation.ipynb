{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "# from mlagents.envs import UnityEnvironment\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d3f901e32abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menvironment_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Environments/Banana.app\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0maca_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_academy_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_init_parameters_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnityTimeOutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36msend_academy_parameters\u001b[0;34m(self, init_parameters)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap_unity_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnityRLInput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnityOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             raise UnityTimeOutException(\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;34m\"\\t The environment does not need user interaction to launch\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;34m\"\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "environment_path = \"Environments/Banana.app\"\n",
    "env = UnityEnvironment(file_name=environment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_QNetwork(nn.Module):\n",
    "    def __init__(self,state_space,action_space,seed,hidden_dims=(32,32),activation_fc=F.relu):\n",
    "        super(Dueling_QNetwork,self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        print('hidden_dims',hidden_dims)\n",
    "        self.input_layer = nn.Linear(state_space,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.value_output = nn.Linear(hidden_dims[-1],1)\n",
    "        self.advantage_output = nn.Linear(hidden_dims[-1],action_space)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        a = self.advantage_output(x)\n",
    "        v = self.value_output(x)\n",
    "        v = v.expand_as(a)\n",
    "        q = v + a - a.mean(1,keepdim=True).expand_as(a)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Tree.\n",
    "3 tiered tree structure containing\n",
    "Root node (Object. sum of all lower values)\n",
    "Intermediate Node (Object. Root as parent, sums a given slice of the priority array)\n",
    "Priority Array (Array of priorities, length buffer_size)\n",
    "\n",
    "The number of Intermediate nodes is calculated by the buffer_size / batch_size.\n",
    "\n",
    "I_episode: current episode of training\n",
    "\n",
    "Index: is calculated by i_episode % buffer_size. This loops the index after exceeding the buffer_size.\n",
    "\n",
    "Indices: (List) of memory/priority entries\n",
    "\n",
    "intermediate_dict: maps index to intermediate node. Since each Intermediate node is responsible \n",
    "for a given slice of the priority array, given a particular index, it will return the Intermediate node\n",
    "'responsible' for that index.\n",
    "\n",
    "## Functions:\n",
    "\n",
    "Add:\n",
    "Calculates the priority of each TD error -> (abs(TD_error)+epsilon)**alpha\n",
    "Stores the priority in the Priority_array.\n",
    "Updates the sum_tree with the new priority\n",
    "\n",
    "Update_Priorities:\n",
    "Updates the index with the latest priority of that sample. As priorities can change over training\n",
    "for a particular experience\n",
    "\n",
    "Sample:\n",
    "Splits the current priority_array based on the number of entries, by the batch_size.\n",
    "Returns the indicies of those samples and the priorities.\n",
    "\n",
    "Propogate:\n",
    "Propogates the new priority value up through the tree\n",
    "\"\"\"\n",
    "\n",
    "class PriorityTree(object):\n",
    "    def __init__(self,buffer_size,batch_size,alpha,epsilon):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_intermediate_nodes = round(buffer_size / batch_size)\n",
    "        self.current_intermediate_node = 0\n",
    "        self.root = Node(None)\n",
    "        self.intermediate_nodes = [Intermediate(self.root,batch_size*x,batch_size*(x+1)) for x in range(self.num_intermediate_nodes)]\n",
    "        self.priority_array = np.zeros(buffer_size)\n",
    "        self.intermediate_dict = {}\n",
    "        for index,node in enumerate(self.intermediate_nodes):\n",
    "            for key in range((batch_size*(index+1))-batch_size,batch_size*(index+1)):\n",
    "                self.intermediate_dict[key] = node\n",
    "        print('Priority Tree: Batch Size {} Buffer size {} Number of intermediate Nodes {}'.format(batch_size,buffer_size,self.num_intermediate_nodes))\n",
    "        \n",
    "    def add(self,TD_error,index):\n",
    "        priority = (abs(TD_error)+self.epsilon)**self.alpha\n",
    "        self.priority_array[index] = priority\n",
    "        # Update sum\n",
    "        propogate(self.intermediate_dict[index],self.priority_array)\n",
    "    \n",
    "    def sample(self,index):\n",
    "        # Sample one experience uniformly from each slice of the priorities\n",
    "        if index >= self.buffer_size:\n",
    "            indicies = [random.sample(list(range(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes)),1)[0] for sample in range(self.batch_size)]\n",
    "        else:\n",
    "            interval = int(index / self.batch_size)\n",
    "            indicies = [random.sample(list(range(sample*interval,(sample+1)*interval)),1)[0] for sample in range(self.batch_size)]\n",
    "#         print('indicies',indicies)\n",
    "        priorities = self.priority_array[indicies]\n",
    "        return priorities,indicies\n",
    "    \n",
    "    def update_priorities(self,TD_errors,indicies):\n",
    "#         print('TD_errors',TD_errors)\n",
    "#         print('TD_errors shape',TD_errors.shape)\n",
    "        priorities = (abs(TD_errors)+self.epsilon)**self.alpha\n",
    "#         print('priorities shape',priorities.shape)\n",
    "#         print('indicies shape',len(indicies))\n",
    "#         print('self.priority_array shape',self.priority_array.shape)\n",
    "        self.priority_array[indicies] = priorities\n",
    "        # Update sum\n",
    "        nodes = [self.intermediate_dict[index] for index in indicies] \n",
    "        intermediate_nodes = set(nodes)\n",
    "        [propogate(node,self.priority_array) for node in intermediate_nodes]\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self,parent):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "            \n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def sum_children(self):\n",
    "        return sum([child.value for child in self.children])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "class Intermediate(Node):\n",
    "    def __init__(self,parent,start,end):\n",
    "        self.parent = parent\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.value = 0\n",
    "        parent.add_child(self)\n",
    "    \n",
    "    def sum_leafs(self,arr):\n",
    "        return np.sum(arr[self.start:self.end])\n",
    "\n",
    "def propogate(node,arr):\n",
    "    if node.parent != None:\n",
    "        node.value = node.sum_leafs(arr)\n",
    "        propogate(node.parent,arr)\n",
    "    else:\n",
    "        node.value = node.sum_children()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Buffer HyperParameters\n",
    "alpha(priority or w) dictates how biased the sampling should be towards the TD error. 0 < a < 1\n",
    "beta(IS) informs the importance of the sample update\n",
    "\n",
    "The paper uses a sum tree to calculate the priority sum in O(log n) time. As such, i've implemented my own version\n",
    "of the sum_tree which i call priority tree.\n",
    "\n",
    "We're increasing beta(IS) from 0.5 to 1 over time\n",
    "alpha(priority) we're holding constant at 0.5\n",
    "\"\"\"\n",
    "\n",
    "class PriorityReplayBuffer(object):\n",
    "    def __init__(self,action_size,buffer_size,batch_size,seed,alpha=0.5,beta=0.5,beta_end=1,beta_duration=1e+5,epsilon=7e-5):\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_duration = beta_duration\n",
    "        self.beta_increment = (beta_end - beta) / beta_duration\n",
    "        self.max_w = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.TD_sum = 0\n",
    "\n",
    "        self.experience = namedtuple('experience',field_names=['state','action','reward','next_state','done','i_episode'])\n",
    "        self.sum_tree = PriorityTree(buffer_size,batch_size,alpha,epsilon)\n",
    "        self.memory = {}\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done,TD_error,i_episode):\n",
    "        e = self.experience(state,action,reward,next_state,done,i_episode)\n",
    "        index = i_episode % self.buffer_size\n",
    "        # add memory to memory and add corresponding priority to the priority tree\n",
    "        self.memory[index] = e\n",
    "        self.sum_tree.add(TD_error,index)\n",
    "\n",
    "    def sample(self,index):\n",
    "        # We times the error by these weights for the updates\n",
    "        # Super inefficient to sum everytime. We could implement the tree sum structure. \n",
    "        # Or we could sum once on the first sample and then keep track of what we add and lose from the buffer.\n",
    "        # priority^a over the sum of the priorities^a = likelyhood of the given choice\n",
    "        # Anneal beta\n",
    "        self.update_beta()\n",
    "        # Get the samples and indicies\n",
    "        priorities,indicies = self.sum_tree.sample(index)\n",
    "        # Normalize with the sum\n",
    "        norm_priorities = priorities / self.sum_tree.root.value\n",
    "        samples = [self.memory[index] for index in indicies]\n",
    "#         samples = list(operator.itemgetter(*self.memory)(indicies))\n",
    "#         samples = self.memory[indicies]\n",
    "        # Importance weights\n",
    "#         print('self.beta',self.beta)\n",
    "#         print('self.beta',self.buffer_size)\n",
    "        importances = [(priority * self.buffer_size)**-self.beta for priority in norm_priorities]\n",
    "        self.max_w = max(self.max_w,max(importances))\n",
    "        # Normalize importance weights\n",
    "#         print('importances',importances)\n",
    "#         print('self.max_w',self.max_w)\n",
    "        norm_importances = [importance / self.max_w for importance in importances]\n",
    "#         print('norm_importances',norm_importances)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in samples if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in samples if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in samples if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in samples if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in samples if e is not None])).float().to(device)\n",
    "        \n",
    "        if index % 4900 == 0:\n",
    "            print('beta',self.beta)\n",
    "            print('self.max_w',self.max_w)\n",
    "            print('len mem',len(self.memory))\n",
    "            print('tree sum',self.sum_tree.root.value)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones),indicies,norm_importances\n",
    "\n",
    "    def update_beta(self):\n",
    "#         print('update_beta')\n",
    "#         print('self.beta_end',self.beta_end)\n",
    "#         print('self.beta_increment',self.beta_increment)\n",
    "        self.beta += self.beta_increment\n",
    "        self.beta = min(self.beta,self.beta_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DQN with Priority Replay, DDQN, and Dueling DQN.\n",
    "\"\"\"\n",
    "class Priority_DQN(object):\n",
    "    def __init__(self,state_space,action_space,seed,update_every,batch_size,buffer_size,min_buffer_size,learning_rate,GAMMA,tau,clip_norm,alpha):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        self.GAMMA = GAMMA\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.clip_norm = clip_norm\n",
    "        \n",
    "        self.qnetwork_local = Dueling_QNetwork(state_space,action_space,seed)\n",
    "        self.qnetwork_target = Dueling_QNetwork(state_space,action_space,seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=learning_rate)\n",
    "        # Initialize replaybuffer\n",
    "        self.memory = PriorityReplayBuffer(action_space,buffer_size,batch_size,seed,alpha)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self,state,action,reward,next_state,done,index):\n",
    "        # Calculate TD error\n",
    "        # Target\n",
    "        current_network_action = self.qnetwork_local(next_state).max(1)[1]\n",
    "        # initial state comes in as (1,4), squeeze to get (4)\n",
    "        target = reward + self.GAMMA*(self.qnetwork_target(next_state).squeeze(0)[current_network_action])\n",
    "        # Local. same rational for squeezing\n",
    "        local = self.qnetwork_local(state).squeeze(0)[action]\n",
    "        TD_error = reward + target - local\n",
    "        # Save the experience\n",
    "        self.memory.add(state,action,reward,next_state,done,TD_error,index)\n",
    "        \n",
    "        # learn from the experience\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.min_buffer_size:\n",
    "                experiences,indicies,weights = self.memory.sample(index)\n",
    "                self.learn(experiences,indicies,weights)\n",
    "        \n",
    "    def act(self,state,eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_space))\n",
    "        \n",
    "    def learn(self,experiences,indicies,weights):\n",
    "        \n",
    "        states,actions,rewards,next_states,dones = experiences\n",
    "        # Local max action\n",
    "        local_next_state_actions = self.qnetwork_local(next_states).max(1)[1].unsqueeze(1)\n",
    "        # Target\n",
    "        target_values = self.qnetwork_target(next_states).detach()\n",
    "        max_target = target_values.gather(1,local_next_state_actions)\n",
    "#         print('max_target size',max_target.size())\n",
    "        max_target *= (1-dones) \n",
    "        targets = rewards + (self.GAMMA*max_target)\n",
    "#         print('targets',targets.size())\n",
    "#         targets = rewards + self.GAMMA*(target_values.gather(1,local_next_state_actions))\n",
    "        # Local\n",
    "        local = self.qnetwork_local(states).gather(1,actions)\n",
    "        TD_error = local - targets\n",
    "        loss = ((torch.tensor(weights) * TD_error)**2*0.5).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(),self.clip_norm)\n",
    "        self.optimizer.step()\n",
    "        # Update the priorities\n",
    "        TD_errors = np.abs(TD_error.squeeze(1).detach().cpu().numpy())\n",
    "        self.memory.sum_tree.update_priorities(TD_errors,indicies)\n",
    "        self.update_target()\n",
    "\n",
    "    # def update_target(self,tau):\n",
    "    #     for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "    #         target_param.data.copy_(local_param.data)\n",
    "        \n",
    "    # Polyak averaging  \n",
    "    def update_target(self):\n",
    "        for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)\n",
    "#         self.qnetwork_local.parameters() = TAU*self.qnetwork_local.parameters() + (1-TAU)*self.qnetwork_target.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,n_episodes=1800, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        Instead of updating target every (int) steps, using Polyak updating of .1 to gradually merge the networks\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    index = 0\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done,index)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            index += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps*eps_decay,eps_end)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "MIN_BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 50\n",
    "ALPHA = 0.6 # 0.7 or 0.6\n",
    "START_BETA = 0.5 # from 0.5-1\n",
    "END_BETA = 1\n",
    "LR = 0.00025\n",
    "EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.01\n",
    "UPDATE_EVERY = 4\n",
    "CLIP_NORM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0)\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "print('Observation Space {}, Action Space {}'.format(nS,nA))\n",
    "seed = 7\n",
    "agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA)\n",
    "\n",
    "scores = train(agent,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Rolling mean plot\n",
    "interval = 25\n",
    "rolling_mean = [np.mean(scores[(slice_*interval):(slice_+1)*interval]) for slice_ in range(math.ceil(len(scores)/interval))]\n",
    "x_axis = np.arange(len(rolling_mean)) * interval\n",
    "plt.plot(x_axis, rolling_mean)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('test'+ str(j)+'.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b989b438395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# load the weights from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b989b438395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Else reinstantiate the environment and other variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "    # load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "# except:  \n",
    "#     # Else reinstantiate the environment and other variables\n",
    "#     env = gym.make('CartPole-v0')\n",
    "#     env.seed(0)\n",
    "#     nA = env.action_space.n\n",
    "#     nS = env.observation_space.shape[0]\n",
    "#     seed = 7\n",
    "#     agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA)\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "\n",
    "for i in range(1):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # save the image\n",
    "        plt.savefig('test'+ str(j)+'.png',bbox_inches='tight')\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a GIF from stored sequence of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Make gif from images\n",
    "path = '/Users/morgan/RL_gif_images'\n",
    "images = []\n",
    "filenames = ['test' + str(i) + '.png' for i in range(500)]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(path +'/'+ filename))\n",
    "imageio.mimsave('Navigation.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
